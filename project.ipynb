{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, random, sys, copy\n",
    "import torch, torch.nn as nn, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from nltk.tokenize import word_tokenize\n",
    "import statistics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import math\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.En.csv')\n",
    "test = pd.read_csv('task_A_En_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today my pop-pop told me I was not ‚Äúforced‚Äù to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>The population spike in Chicago in 9 months is...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>You'd think in the second to last English clas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>I‚Äôm finally surfacing after a holiday to Scotl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>Couldn't be prouder today. Well done to every ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3467</th>\n",
       "      <td>Overheard as my 13 year old games with a frien...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3468 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  sarcastic\n",
       "0     The only thing I got from college is a caffein...          1\n",
       "1     I love it when professors draw a big question ...          1\n",
       "2     Remember the hundred emails from companies whe...          1\n",
       "3     Today my pop-pop told me I was not ‚Äúforced‚Äù to...          1\n",
       "4     @VolphanCarol @littlewhitty @mysticalmanatee I...          1\n",
       "...                                                 ...        ...\n",
       "3463  The population spike in Chicago in 9 months is...          0\n",
       "3464  You'd think in the second to last English clas...          0\n",
       "3465  I‚Äôm finally surfacing after a holiday to Scotl...          0\n",
       "3466  Couldn't be prouder today. Well done to every ...          0\n",
       "3467  Overheard as my 13 year old games with a frien...          0\n",
       "\n",
       "[3468 rows x 2 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[['tweet', 'sarcastic']]\n",
    "train.rename(columns={'tweet': 'text'}, inplace=True)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Size on the the Toulouse team, That pack is mo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pinball!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So the Scottish Government want people to get ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>villainous pro tip : change the device name on...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I would date any of these men ü•∫</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>I‚Äôve just seen this and felt it deserved a Ret...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>Omg how an earth is that a pen !!! ü§°</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>Bringing Kanye and drake to a tl near you</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>I love it when women are referred to as \"girl ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>The fact that people still don't get that you ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  sarcastic\n",
       "0     Size on the the Toulouse team, That pack is mo...          0\n",
       "1                                              Pinball!          0\n",
       "2     So the Scottish Government want people to get ...          1\n",
       "3     villainous pro tip : change the device name on...          0\n",
       "4                       I would date any of these men ü•∫          0\n",
       "...                                                 ...        ...\n",
       "1395  I‚Äôve just seen this and felt it deserved a Ret...          0\n",
       "1396               Omg how an earth is that a pen !!! ü§°          0\n",
       "1397          Bringing Kanye and drake to a tl near you          0\n",
       "1398  I love it when women are referred to as \"girl ...          1\n",
       "1399  The fact that people still don't get that you ...          1\n",
       "\n",
       "[1400 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove Dict Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
      "\n",
      "Loaded 400000 words from glove\n",
      "0\n",
      "[ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "glove_file = 'glove.6B.50d.txt'\n",
    "\n",
    "embeddings_dict = {}\n",
    "\n",
    "with open(glove_file, 'r', encoding='utf8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            print(line)\n",
    "        line = line.strip().split(' ')\n",
    "        word = line[0]\n",
    "        embed = np.asarray(line[1:], \"float\")\n",
    "\n",
    "        embeddings_dict[word] = embed\n",
    "\n",
    "print('Loaded {} words from glove'.format(len(embeddings_dict)))\n",
    "\n",
    "embedding_matrix = np.zeros((len(embeddings_dict)+1, 50)) #add 1 for padding\n",
    "\n",
    "word2id = {}\n",
    "for i, word in enumerate(embeddings_dict.keys()):\n",
    "\n",
    "    word2id[word] = i                                #Map each word to an index\n",
    "    embedding_matrix[i] = embeddings_dict[word]      #That index holds the Glove embedding in the embedding matrix\n",
    "\n",
    "# Our joint vocabulary for both models / sanity check to see if we've loaded it correctly:\n",
    "print(word2id['the'])\n",
    "print(embedding_matrix[word2id['the']])\n",
    "\n",
    "word2id['<pad>'] = embedding_matrix.shape[0] - 1\n",
    "print(embedding_matrix[word2id['<pad>']])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarcasmDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, sarcastic=None, nonSarcastic=None, split=None, word2id=None, finalized_data=None, data_limit=250, max_length=256):\n",
    "        \"\"\"\n",
    "        :param sarcastic: The sarcastic dataset\n",
    "        :param nonSarcastic: The non-sarcastic dataset\n",
    "        :param split: Train or test\n",
    "        :param word2id: The generated glove word2id dictionary\n",
    "        :param finalized_data: We'll use this to initialize a validation set without reloading the data.\n",
    "        :param data_limit: Limiter on the number of examples we load\n",
    "        :param max_length: Maximum length of the sequence\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_limit = data_limit\n",
    "        self.max_length = max_length\n",
    "        self.word2id = word2id\n",
    "\n",
    "        if finalized_data:\n",
    "            self.data = finalized_data\n",
    "\n",
    "        else:\n",
    "\n",
    "            pos_examples = sarcastic\n",
    "            neg_examples = nonSarcastic\n",
    "\n",
    "            pos_examples_tokenized = [(ids, 1) for ids in self.tokenize(pos_examples)]\n",
    "            neg_examples_tokenized = [(ids, 0) for ids in self.tokenize(neg_examples)]\n",
    "\n",
    "            self.data = pos_examples_tokenized + neg_examples_tokenized\n",
    "\n",
    "            random.shuffle(self.data)\n",
    "\n",
    "    def tokenize(self, examples):\n",
    "\n",
    "        example_ids = []\n",
    "        misses = 0              # Count the number of tokens in our dataset which are not covered by glove -- i.e. percentage of unk tokens\n",
    "        total = 0\n",
    "        for example in examples:\n",
    "            tokens = word_tokenize(example)\n",
    "            ids = []\n",
    "            for tok in tokens:\n",
    "                if tok in word2id:\n",
    "                    ids.append(word2id[tok])\n",
    "                else:\n",
    "                    misses += 1\n",
    "                    ids.append(word2id['unk'])\n",
    "                total += 1\n",
    "\n",
    "            if len(ids) >= self.max_length:\n",
    "                ids = ids[:self.max_length]\n",
    "            else:\n",
    "                ids = ids + [word2id['<pad>']]*(self.max_length - len(ids))\n",
    "            example_ids.append(torch.tensor(ids))\n",
    "        print('Missed {} out of {} words -- {:.2f}%'.format(misses, total, misses/total))\n",
    "        return example_ids\n",
    "\n",
    "    def generate_validation_split(self, ratio=0.8):\n",
    "\n",
    "        split_idx = int(ratio * len(self.data))\n",
    "\n",
    "        # Take a chunk of the processed data, and return it in order to initialize a validation dataset\n",
    "        validation_split = self.data[split_idx:]\n",
    "\n",
    "        #We'll remove this data from the training data to prevent leakage\n",
    "        self.data = self.data[:split_idx]\n",
    "\n",
    "        return validation_split\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Glove Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveModel(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_embedding, hidden_dim, num_hidden_layers, max_length=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(pretrained_embedding))\n",
    "        self.hidden_layer_1 = nn.Linear(pretrained_embedding.shape[1], hidden_dim)\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_hidden_layers - 1)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        embedding = self.embedding(input).squeeze(1)\n",
    "        embedding = torch.sum(embedding, dim=1)\n",
    "\n",
    "        hidden = self.relu(self.hidden_layer_1(embedding))\n",
    "        for layer in self.hidden_layers:\n",
    "            hidden = self.relu(layer(hidden))\n",
    "\n",
    "        output = self.output_layer(hidden)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, valid_dataloader):\n",
    "\n",
    "    sigmoid = nn.Sigmoid()\n",
    "\n",
    "    total_correct = 0\n",
    "    total_examples = len(valid_dataloader)\n",
    "\n",
    "    for x,y in valid_dataloader:\n",
    "\n",
    "        x = x.unsqueeze(1)\n",
    "        output = sigmoid(model(x))\n",
    "\n",
    "        if (output < 0.5 and y == 0) or (output >= 0.5 and y == 1):\n",
    "            total_correct += 1\n",
    "\n",
    "    accuracy = total_correct / total_examples\n",
    "    print('accuracy: {}'.format(accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification(model, train_dataset, valid_dataset, accuracyArray, epochs=100, batch_size=32, print_frequency=100):\n",
    "    criteria = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters())            \n",
    "    \n",
    "\n",
    "    epochs = epochs\n",
    "    batch_size = batch_size\n",
    "    print_frequency = print_frequency\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print('### Epoch: ' + str(i+1) + ' ###')\n",
    "\n",
    "        model.train()\n",
    "        avg_loss = 0\n",
    "\n",
    "        for step, data in enumerate(train_dataloader):\n",
    "\n",
    "            x, y = data\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            model_output = model(x)\n",
    "\n",
    "            loss = criteria(model_output.squeeze(1), y.float())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "            if step % print_frequency == 1:\n",
    "                print('epoch: {} batch: {} loss: {}'.format(\n",
    "                    i,\n",
    "                    step,\n",
    "                    avg_loss / print_frequency\n",
    "                ))\n",
    "                avg_loss = 0\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            accuracyArray.append(predict(model, valid_dataloader))\n",
    "            predict(model, valid_dataloader)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train[\"sarcastic\"] == 1].to_csv(\"trainSarcastic.csv\", index=False)\n",
    "train[train[\"sarcastic\"] == 0].to_csv(\"trainNonSarcastic.csv\", index=False)\n",
    "test.to_csv(\"testDataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missed 1 out of 18 words -- 0.06%\n",
      "Missed 2 out of 21 words -- 0.10%\n",
      "Loaded 31 train examples\n",
      "Loaded 8 validation examples\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SarcasmDataset(\"trainSarcastic.csv\", \"trainNonSarcastic.csv\", 'train', word2id)\n",
    "validation_examples = train_dataset.generate_validation_split()\n",
    "print('Loaded {} train examples'.format(train_dataset.__len__()))\n",
    "\n",
    "valid_dataset = SarcasmDataset(finalized_data=validation_examples, word2id=word2id)\n",
    "print('Loaded {} validation examples'.format(valid_dataset.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Epoch: 1 ###\n",
      "accuracy: 0.5\n",
      "accuracy: 0.5\n",
      "### Epoch: 2 ###\n",
      "accuracy: 0.5\n",
      "accuracy: 0.5\n",
      "### Epoch: 3 ###\n",
      "accuracy: 0.5\n",
      "accuracy: 0.5\n",
      "### Epoch: 4 ###\n",
      "accuracy: 0.5\n",
      "accuracy: 0.5\n",
      "### Epoch: 5 ###\n",
      "accuracy: 0.5\n",
      "accuracy: 0.5\n",
      "### Epoch: 6 ###\n",
      "accuracy: 0.5\n",
      "accuracy: 0.5\n",
      "### Epoch: 7 ###\n",
      "accuracy: 0.5\n",
      "accuracy: 0.5\n",
      "### Epoch: 8 ###\n",
      "accuracy: 0.5\n",
      "accuracy: 0.5\n",
      "### Epoch: 9 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 10 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 11 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 12 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 13 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 14 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 15 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 16 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 17 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 18 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 19 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 20 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 21 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 22 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 23 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 24 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 25 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 26 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 27 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 28 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 29 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 30 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 31 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 32 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 33 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 34 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 35 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 36 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 37 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 38 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 39 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 40 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 41 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 42 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 43 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 44 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 45 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 46 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 47 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 48 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 49 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 50 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 51 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 52 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 53 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 54 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 55 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 56 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 57 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 58 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 59 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 60 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 61 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 62 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 63 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 64 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 65 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 66 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 67 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 68 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 69 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 70 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 71 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 72 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 73 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 74 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 75 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 76 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 77 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 78 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 79 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 80 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 81 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 82 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 83 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 84 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 85 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 86 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 87 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 88 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 89 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 90 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 91 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 92 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 93 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 94 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 95 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 96 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 97 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 98 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 99 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n",
      "### Epoch: 100 ###\n",
      "accuracy: 0.25\n",
      "accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "accuracyArray = []\n",
    "glove_model = GloveModel(embedding_matrix, 100, 5)\n",
    "train_classification(glove_model, train_dataset, valid_dataset, accuracyArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missed 1 out of 15 words -- 0.07%\n",
      "Missed 0 out of 4 words -- 0.00%\n",
      "Glove model accuracy: \n",
      "accuracy: 0.5263157894736842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5263157894736842"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = SarcasmDataset(\"testDataset.csv\", 'test', word2id)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print('Glove model accuracy: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
